apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-community-server-config
  namespace: metrics
data:
  alerting_rules.yml: |
    groups:
      - name: self-monitoring
        rules:
        - alert: PrometheusNotConnectedToAlertmanager
          expr: prometheus_notifications_alertmanagers_discovered < 1
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: "Prometheus not connected to alertmanager (instance {{ $labels.instance }})"
            description: "Prometheus cannot connect the alertmanager\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
        - alert: PrometheusConfigurationReloadFailure
          expr: prometheus_config_last_reload_successful != 1
          for: 0m
          labels:
            severity: warning
          annotations:
            summary: "Prometheus configuration reload failure (instance {{ $labels.instance }})"
            description: "Prometheus configuration reload error\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
        - alert: PrometheusTooManyRestarts
          expr: changes(process_start_time_seconds{job=~"prometheus|pushgateway|alertmanager"}[15m]) > 2
          for: 0m
          labels:
            severity: warning
          annotations:
            summary: "Prometheus too many restarts (instance {{ $labels.instance }})"
            description: "Prometheus has restarted more than twice in the last 15 minutes. It might be crashlooping.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
        - alert: PrometheusAlertmanagerConfigurationReloadFailure
          expr: alertmanager_config_last_reload_successful != 1
          for: 0m
          labels:
            severity: warning
          annotations:
            summary: "Prometheus AlertManager configuration reload failure (instance {{ $labels.instance }})"
            description: "AlertManager configuration reload error\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
        - alert: PrometheusAlertmanagerDeadManSwitch
          expr: vector(1)
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: "Prometheus AlertManager dead man switch (instance {{ $labels.instance }})"
            description: "Prometheus DeadManSwitch is an always-firing alert. It's used as an end-to-end test of Prometheus through the Alertmanager.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
        - alert: PrometheusNotConnectedToAlertmanager
          expr: prometheus_notifications_alertmanagers_discovered < 1
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: "Prometheus not connected to alertmanager (instance {{ $labels.instance }})"
            description: "Prometheus cannot connect the alertmanager\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
        - alert: PrometheusRuleEvaluationFailures
          expr: increase(prometheus_rule_evaluation_failures_total[3m]) > 0
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: "Prometheus rule evaluation failures (instance {{ $labels.instance }})"
            description: "Prometheus encountered {{ $value }} rule evaluation failures, leading to potentially ignored alerts.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
        - alert: PrometheusAlertmanagerNotificationFailing
          expr: rate(alertmanager_notifications_failed_total[1m]) > 0
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: "Prometheus AlertManager notification failing (instance {{ $labels.instance }})"
            description: "Alertmanager is failing sending notifications\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
        - alert: PrometheusTsdbCheckpointCreationFailures
          expr: increase(prometheus_tsdb_checkpoint_creations_failed_total[1m]) > 0
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: "Prometheus TSDB checkpoint creation failures (instance {{ $labels.instance }})"
            description: "Prometheus encountered {{ $value }} checkpoint creation failures\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
        - alert: PrometheusTsdbCompactionsFailed
          expr: increase(prometheus_tsdb_compactions_failed_total[1m]) > 0
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: "Prometheus TSDB compactions failed (instance {{ $labels.instance }})"
            description: "Prometheus encountered {{ $value }} TSDB compactions failures\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
        - alert: PrometheusTsdbHeadTruncationsFailed
          expr: increase(prometheus_tsdb_head_truncations_failed_total[1m]) > 0
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: "Prometheus TSDB head truncations failed (instance {{ $labels.instance }})"
            description: "Prometheus encountered {{ $value }} TSDB head truncation failures\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
        - alert: PrometheusTsdbReloadFailures
          expr: increase(prometheus_tsdb_reloads_failures_total[1m]) > 0
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: "Prometheus TSDB reload failures (instance {{ $labels.instance }})"
            description: "Prometheus encountered {{ $value }} TSDB reload failures\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
        - alert: PrometheusTsdbWalCorruptions
          expr: increase(prometheus_tsdb_wal_corruptions_total[1m]) > 0
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: "Prometheus TSDB WAL corruptions (instance {{ $labels.instance }})"
            description: "Prometheus encountered {{ $value }} TSDB WAL corruptions\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
        - alert: PrometheusTsdbWalTruncationsFailed
          expr: increase(prometheus_tsdb_wal_truncations_failed_total[1m]) > 0
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: "Prometheus TSDB WAL truncations failed (instance {{ $labels.instance }})"
            description: "Prometheus encountered {{ $value }} TSDB WAL truncation failures\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
      - name: kube-state-metrics
        rules:
        - alert: KubernetesNodeReady
          expr: kube_node_status_condition{condition="Ready",status="true"} == 0
          for: 10m
          labels:
            severity: critical
          annotations:
            summary: "Kubernetes Node ready (instance {{ $labels.instance }})"
            description: "Node {{ $labels.node }} has been unready for a long time\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
        - alert: KubernetesJobFailed
          expr: kube_job_status_failed > 0
          for: 0m
          labels:
            severity: warning
          annotations:
            summary: "Kubernetes Job failed (instance {{ $labels.instance }})"
            description: "Job {{$labels.namespace}}/{{$labels.exported_job}} failed to complete\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
        - alert: KubernetesPersistentvolumeclaimPending
          expr: kube_persistentvolumeclaim_status_phase{phase="Pending"} == 1
          for: 2m
          labels:
            severity: warning
          annotations:
            summary: "Kubernetes PersistentVolumeClaim pending (instance {{ $labels.instance }})"
            description: "PersistentVolumeClaim {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} is pending\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
        - alert: KubernetesVolumeOutOfDiskSpace
          expr: kubelet_volume_stats_available_bytes / kubelet_volume_stats_capacity_bytes * 100 < 10
          for: 2m
          labels:
            severity: warning
          annotations:
            summary: "Kubernetes Volume out of disk space (instance {{ $labels.instance }})"
            description: "Volume is almost full (< 10% left)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
        - alert: KubernetesVolumeFullInFourDays
          expr: predict_linear(kubelet_volume_stats_available_bytes[6h], 4 * 24 * 3600) < 0
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: "Kubernetes Volume full in four days (instance {{ $labels.instance }})"
            description: "{{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} is expected to fill up within four days. Currently {{ $value | humanize }}% is available.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
        - alert: KubernetesPersistentvolumeError
          expr: kube_persistentvolume_status_phase{phase=~"Failed|Pending", job="kube-state-metrics"} > 0
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: "Kubernetes PersistentVolume error (instance {{ $labels.instance }})"
            description: "Persistent volume is in bad state\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
        - alert: KubernetesStatefulsetDown
          expr: (kube_statefulset_status_replicas_ready / kube_statefulset_status_replicas_current) != 1
          for: 1m
          labels:
            severity: critical
          annotations:
            summary: "Kubernetes StatefulSet down (instance {{ $labels.instance }})"
            description: "A StatefulSet went down\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
        - alert: KubernetesHpaScalingAbility
          expr: kube_hpa_status_condition{status="false", condition="AbleToScale"} == 1
          for: 2m
          labels:
            severity: warning
          annotations:
            summary: "Kubernetes HPA scaling ability (instance {{ $labels.instance }})"
            description: "Pod is unable to scale\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
        - alert: KubernetesHpaMetricAvailability
          expr: kube_hpa_status_condition{status="false", condition="ScalingActive"} == 1
          for: 0m
          labels:
            severity: warning
          annotations:
            summary: "Kubernetes HPA metric availability (instance {{ $labels.instance }})"
            description: "HPA is not able to collect metrics\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
        - alert: KubernetesHpaScaleCapability
          expr: kube_hpa_status_desired_replicas >= kube_hpa_spec_max_replicas
          for: 2m
          labels:
            severity: info
          annotations:
            summary: "Kubernetes HPA scale capability (instance {{ $labels.instance }})"
            description: "The maximum number of desired Pods has been hit\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
        - alert: KubernetesPodNotHealthy
          expr: min_over_time(sum by (namespace, pod) (kube_pod_status_phase{phase=~"Pending|Unknown|Failed"})[1h:]) > 0
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: "Kubernetes Pod not healthy (instance {{ $labels.instance }})"
            description: "Pod has been in a non-ready state for longer than an hour.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
        - alert: KubernetesPodCrashLooping
          expr: increase(kube_pod_container_status_restarts_total[1m]) > 3
          for: 2m
          labels:
            severity: warning
          annotations:
            summary: "Kubernetes pod crash looping (instance {{ $labels.instance }})"
            description: "Pod {{ $labels.pod }} is crash looping\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
        - alert: KubernetesReplicassetMismatch
          expr: kube_replicaset_spec_replicas != kube_replicaset_status_ready_replicas
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "Kubernetes ReplicasSet mismatch (instance {{ $labels.instance }})"
            description: "Deployment Replicas mismatch\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
        - alert: KubernetesDeploymentReplicasMismatch
          expr: kube_deployment_spec_replicas != kube_deployment_status_replicas_available
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "Kubernetes Deployment replicas mismatch (instance {{ $labels.instance }})"
            description: "Deployment Replicas mismatch\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
        - alert: KubernetesStatefulsetReplicasMismatch
          expr: kube_statefulset_status_replicas_ready != kube_statefulset_status_replicas
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "Kubernetes StatefulSet replicas mismatch (instance {{ $labels.instance }})"
            description: "A StatefulSet does not match the expected number of replicas.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
        - alert: KubernetesJobSlowCompletion
          expr: kube_job_spec_completions - kube_job_status_succeeded > 0
          for: 12h
          labels:
            severity: critical
          annotations:
            summary: "Kubernetes job slow completion (instance {{ $labels.instance }})"
            description: "Kubernetes Job {{ $labels.namespace }}/{{ $labels.job_name }} did not complete in time.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
        - alert: KubernetesApiServerErrors
          expr: sum(rate(apiserver_request_count{job="apiserver",code=~"^(?:5..)$"}[1m])) / sum(rate(apiserver_request_count{job="apiserver"}[1m])) * 100 > 3
          for: 2m
          labels:
            severity: critical
          annotations:
            summary: "Kubernetes API server errors (instance {{ $labels.instance }})"
            description: "Kubernetes API server is experiencing high error rate\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
        - alert: KubernetesApiClientErrors
          expr: (sum(rate(rest_client_requests_total{code=~"(4|5).."}[1m])) by (instance, job) / sum(rate(rest_client_requests_total[1m])) by (instance, job)) * 100 > 1
          for: 2m
          labels:
            severity: critical
          annotations:
            summary: "Kubernetes API client errors (instance {{ $labels.instance }})"
            description: "Kubernetes API client is experiencing high error rate\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
        - alert: KubernetesApiServerLatency
          expr: histogram_quantile(0.99, sum(rate(apiserver_request_latencies_bucket{subresource!="log",verb!~"^(?:CONNECT|WATCHLIST|WATCH|PROXY)$"} [10m])) WITHOUT (instance, resource)) / 1e+06 > 1
          for: 2m
          labels:
            severity: warning
          annotations:
            summary: "Kubernetes API server latency (instance {{ $labels.instance }})"
            description: "Kubernetes API server has a 99th percentile latency of {{ $value }} seconds for {{ $labels.verb }} {{ $labels.resource }}.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
      - name: istio-alerting
        rules:
        - alert: IstioKubernetesGatewayAvailabilityDrop
          expr: min(kube_deployment_status_replicas_available{deployment="istio-ingressgateway", namespace="istio-system"}) without (instance, pod) < 2
          for: 1m
          labels:
            severity: warning
          annotations:
            summary: "Istio Kubernetes gateway availability drop (instance {{ $labels.instance }})"
            description: "Gateway pods have dropped. Inbound traffic will likely be affected.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
        - alert: IstioPilotHighTotalRequestRate
          expr: sum(rate(pilot_xds_push_errors[1m])) / sum(rate(pilot_xds_pushes[1m])) * 100 > 5
          for: 1m
          labels:
            severity: warning
          annotations:
            summary: "Istio Pilot high total request rate (instance {{ $labels.instance }})"
            description: "Number of Istio Pilot push errors is too high (> 5%). Envoy sidecars might have outdated configuration.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
        - alert: IstioMixerPrometheusDispatchesLow
          expr: sum(rate(mixer_runtime_dispatches_total{adapter=~"prometheus"}[1m])) < 180
          for: 1m
          labels:
            severity: warning
          annotations:
            summary: "Istio Mixer Prometheus dispatches low (instance {{ $labels.instance }})"
            description: "Number of Mixer dispatches to Prometheus is too low. Istio metrics might not be being exported properly.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
        - alert: IstioHighTotalRequestRate
          expr: sum(rate(istio_requests_total{reporter="destination"}[5m])) > 1000
          for: 2m
          labels:
            severity: warning
          annotations:
            summary: "Istio high total request rate (instance {{ $labels.instance }})"
            description: "Global request rate in the service mesh is unusually high.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
        - alert: IstioLowTotalRequestRate
          expr: sum(rate(istio_requests_total{reporter="destination"}[5m])) < 100
          for: 2m
          labels:
            severity: warning
          annotations:
            summary: "Istio low total request rate (instance {{ $labels.instance }})"
            description: "Global request rate in the service mesh is unusually low.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
        - alert: IstioHigh4xxErrorRate
          expr: sum(rate(istio_requests_total{reporter="destination", response_code=~"4.*"}[5m])) / sum(rate(istio_requests_total{reporter="destination"}[5m])) * 100 > 5
          for: 1m
          labels:
            severity: warning
          annotations:
            summary: "Istio high 4xx error rate (instance {{ $labels.instance }})"
            description: "High percentage of HTTP 5xx responses in Istio (> 5%).\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
        - alert: IstioHigh5xxErrorRate
          expr: sum(rate(istio_requests_total{reporter="destination", response_code=~"5.*"}[5m])) / sum(rate(istio_requests_total{reporter="destination"}[5m])) * 100 > 5
          for: 1m
          labels:
            severity: warning
          annotations:
            summary: "Istio high 5xx error rate (instance {{ $labels.instance }})"
            description: "High percentage of HTTP 5xx responses in Istio (> 5%).\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
        - alert: IstioHighRequestLatency
          expr: rate(istio_request_duration_milliseconds_sum[1m]) / rate(istio_request_duration_milliseconds_count[1m]) > 0.1
          for: 1m
          labels:
            severity: warning
          annotations:
            summary: "Istio high request latency (instance {{ $labels.instance }})"
            description: "Istio average requests execution is longer than 100ms.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
        - alert: IstioLatency99Percentile
          expr: histogram_quantile(0.99, rate(istio_request_duration_milliseconds_bucket[1m])) > 1
          for: 1m
          labels:
            severity: warning
          annotations:
            summary: "Istio latency 99 percentile (instance {{ $labels.instance }})"
            description: "Istio 1% slowest resquests are longer than 1s.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
  alerts: |
    {}
  prometheus.yml: |
    global:
      evaluation_interval: 20s
      scrape_interval: 20s
      scrape_timeout: 10s
    rule_files:
    - /etc/config/recording_rules.yml
    - /etc/config/alerting_rules.yml
    - /etc/config/rules
    - /etc/config/alerts
    scrape_configs:
    - job_name: prometheus
      static_configs:
      - targets:
        - localhost:9090
    - bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      job_name: kubernetes-apiservers
      kubernetes_sd_configs:
      - role: endpoints
      relabel_configs:
      - action: keep
        regex: default;kubernetes;https
        source_labels:
        - __meta_kubernetes_namespace
        - __meta_kubernetes_service_name
        - __meta_kubernetes_endpoint_port_name
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        insecure_skip_verify: true
    - bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      job_name: kubernetes-nodes
      kubernetes_sd_configs:
      - role: node
      relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
      - replacement: kubernetes.default.svc:443
        target_label: __address__
      - regex: (.+)
        replacement: /api/v1/nodes/$1/proxy/metrics
        source_labels:
        - __meta_kubernetes_node_name
        target_label: __metrics_path__
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        insecure_skip_verify: true
    - bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      job_name: kubernetes-nodes-cadvisor
      kubernetes_sd_configs:
      - role: node
      relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
      - replacement: kubernetes.default.svc:443
        target_label: __address__
      - regex: (.+)
        replacement: /api/v1/nodes/$1/proxy/metrics/cadvisor
        source_labels:
        - __meta_kubernetes_node_name
        target_label: __metrics_path__
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        insecure_skip_verify: true
    - job_name: kubernetes-service-endpoints
      kubernetes_sd_configs:
      - role: endpoints
      relabel_configs:
      - action: keep
        regex: true
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_scrape
      - action: replace
        regex: (https?)
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_scheme
        target_label: __scheme__
      - action: replace
        regex: (.+)
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_path
        target_label: __metrics_path__
      - action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        source_labels:
        - __address__
        - __meta_kubernetes_service_annotation_prometheus_io_port
        target_label: __address__
      - action: labelmap
        regex: __meta_kubernetes_service_label_(.+)
      - action: replace
        source_labels:
        - __meta_kubernetes_namespace
        target_label: kubernetes_namespace
      - action: replace
        source_labels:
        - __meta_kubernetes_service_name
        target_label: kubernetes_name
      - action: replace
        source_labels:
        - __meta_kubernetes_pod_node_name
        target_label: kubernetes_node
    - job_name: kubernetes-service-endpoints-slow
      kubernetes_sd_configs:
      - role: endpoints
      relabel_configs:
      - action: keep
        regex: true
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_scrape_slow
      - action: replace
        regex: (https?)
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_scheme
        target_label: __scheme__
      - action: replace
        regex: (.+)
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_path
        target_label: __metrics_path__
      - action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        source_labels:
        - __address__
        - __meta_kubernetes_service_annotation_prometheus_io_port
        target_label: __address__
      - action: labelmap
        regex: __meta_kubernetes_service_label_(.+)
      - action: replace
        source_labels:
        - __meta_kubernetes_namespace
        target_label: kubernetes_namespace
      - action: replace
        source_labels:
        - __meta_kubernetes_service_name
        target_label: kubernetes_name
      - action: replace
        source_labels:
        - __meta_kubernetes_pod_node_name
        target_label: kubernetes_node
      scrape_interval: 5m
      scrape_timeout: 30s
    - honor_labels: true
      job_name: prometheus-pushgateway
      kubernetes_sd_configs:
      - role: service
      relabel_configs:
      - action: keep
        regex: pushgateway
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_probe
    - job_name: kubernetes-services
      kubernetes_sd_configs:
      - role: service
      metrics_path: /probe
      params:
        module:
        - http_2xx
      relabel_configs:
      - action: keep
        regex: true
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_probe
      - source_labels:
        - __address__
        target_label: __param_target
      - replacement: blackbox
        target_label: __address__
      - source_labels:
        - __param_target
        target_label: instance
      - action: labelmap
        regex: __meta_kubernetes_service_label_(.+)
      - source_labels:
        - __meta_kubernetes_namespace
        target_label: kubernetes_namespace
      - source_labels:
        - __meta_kubernetes_service_name
        target_label: kubernetes_name
    - job_name: kubernetes-pods
      kubernetes_sd_configs:
      - role: pod
      relabel_configs:
      - action: keep
        regex: true
        source_labels:
        - __meta_kubernetes_pod_annotation_prometheus_io_scrape
      - action: replace
        regex: (https?)
        source_labels:
        - __meta_kubernetes_pod_annotation_prometheus_io_scheme
        target_label: __scheme__
      - action: replace
        regex: (.+)
        source_labels:
        - __meta_kubernetes_pod_annotation_prometheus_io_path
        target_label: __metrics_path__
      - action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        source_labels:
        - __address__
        - __meta_kubernetes_pod_annotation_prometheus_io_port
        target_label: __address__
      - action: labelmap
        regex: __meta_kubernetes_pod_label_(.+)
      - action: replace
        source_labels:
        - __meta_kubernetes_namespace
        target_label: kubernetes_namespace
      - action: replace
        source_labels:
        - __meta_kubernetes_pod_name
        target_label: kubernetes_pod_name
      - action: drop
        regex: Pending|Succeeded|Failed
        source_labels:
        - __meta_kubernetes_pod_phase
    - job_name: kubernetes-pods-slow
      kubernetes_sd_configs:
      - role: pod
      relabel_configs:
      - action: keep
        regex: true
        source_labels:
        - __meta_kubernetes_pod_annotation_prometheus_io_scrape_slow
      - action: replace
        regex: (https?)
        source_labels:
        - __meta_kubernetes_pod_annotation_prometheus_io_scheme
        target_label: __scheme__
      - action: replace
        regex: (.+)
        source_labels:
        - __meta_kubernetes_pod_annotation_prometheus_io_path
        target_label: __metrics_path__
      - action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        source_labels:
        - __address__
        - __meta_kubernetes_pod_annotation_prometheus_io_port
        target_label: __address__
      - action: labelmap
        regex: __meta_kubernetes_pod_label_(.+)
      - action: replace
        source_labels:
        - __meta_kubernetes_namespace
        target_label: kubernetes_namespace
      - action: replace
        source_labels:
        - __meta_kubernetes_pod_name
        target_label: kubernetes_pod_name
      - action: drop
        regex: Pending|Succeeded|Failed
        source_labels:
        - __meta_kubernetes_pod_phase
      scrape_interval: 5m
      scrape_timeout: 30s
    - job_name: 'istio-prometheus'
      honor_labels: true
      metrics_path: '/federate'
      kubernetes_sd_configs:
      - role: pod
        namespaces:
          names: ['istio-system']
      metric_relabel_configs:
      - source_labels: [__name__]
        regex: 'workload:(.*)'
        target_label: __name__
        action: replace
      params:
        'match[]':
        - '{__name__=~"workload:(.*)"}'
        - '{__name__=~"pilot(.*)"}'
    alerting:
      alertmanagers:
      - kubernetes_sd_configs:
          - role: pod
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        relabel_configs:
        - source_labels: [__meta_kubernetes_namespace]
          regex: istio-system
          action: keep
        - source_labels: [__meta_kubernetes_pod_label_app]
          regex: prometheus
          action: keep
        - source_labels: [__meta_kubernetes_pod_label_component]
          regex: alertmanager
          action: keep
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_probe]
          regex: .*
          action: keep
        - source_labels: [__meta_kubernetes_pod_container_port_number]
          regex: "9093"
          action: keep
  recording_rules.yml: |
    groups:
      - name: "istio.recording-rules"
        interval: 5s
        rules:
        - record: "workload:istio_requests_total"
          expr: |
            sum without(instance, namespace, pod) (istio_requests_total)
        - record: "workload:istio_request_duration_milliseconds_count"
          expr: |
            sum without(instance, namespace, pod) (istio_request_duration_milliseconds_count)
        - record: "workload:istio_request_duration_milliseconds_sum"
          expr: |
            sum without(instance, namespace, pod) (istio_request_duration_milliseconds_sum)
        - record: "workload:istio_request_duration_milliseconds_bucket"
          expr: |
            sum without(instance, namespace, pod) (istio_request_duration_milliseconds_bucket)
        - record: "workload:istio_request_bytes_count"
          expr: |
            sum without(instance, namespace, pod) (istio_request_bytes_count)
        - record: "workload:istio_request_bytes_sum"
          expr: |
            sum without(instance, namespace, pod) (istio_request_bytes_sum)
        - record: "workload:istio_request_bytes_bucket"
          expr: |
            sum without(instance, namespace, pod) (istio_request_bytes_bucket)
        - record: "workload:istio_response_bytes_count"
          expr: |
            sum without(instance, namespace, pod) (istio_response_bytes_count)
        - record: "workload:istio_response_bytes_sum"
          expr: |
            sum without(instance, namespace, pod) (istio_response_bytes_sum)
        - record: "workload:istio_response_bytes_bucket"
          expr: |
            sum without(instance, namespace, pod) (istio_response_bytes_bucket)
        - record: "workload:istio_tcp_connections_opened_total"
          expr: |
            sum without(instance, namespace, pod) (istio_tcp_connections_opened_total)
        - record: "workload:istio_tcp_connections_closed_total"
          expr: |
            sum without(instance, namespace, pod) (istio_tcp_connections_closed_total)
        - record: "workload:istio_tcp_sent_bytes_total_count"
          expr: |
            sum without(instance, namespace, pod) (istio_tcp_sent_bytes_total_count)
        - record: "workload:istio_tcp_sent_bytes_total_sum"
          expr: |
            sum without(instance, namespace, pod) (istio_tcp_sent_bytes_total_sum)
        - record: "workload:istio_tcp_sent_bytes_total_bucket"
          expr: |
            sum without(instance, namespace, pod) (istio_tcp_sent_bytes_total_bucket)
        - record: "workload:istio_tcp_received_bytes_total_count"
          expr: |
            sum without(instance, namespace, pod) (istio_tcp_received_bytes_total_count)
        - record: "workload:istio_tcp_received_bytes_total_sum"
          expr: |
            sum without(instance, namespace, pod) (istio_tcp_received_bytes_total_sum)
        - record: "workload:istio_tcp_received_bytes_total_bucket"
          expr: |
            sum without(instance, namespace, pod) (istio_tcp_received_bytes_total_bucket)
        - record: "istio:istio_requests:by_destination_service:rate1m"
          expr: |
            sum(irate(istio_requests_total{reporter="destination"}[1m]))
            by (
              destination_canonical_service,
              destination_workload_namespace
            )
        - record: "istio:istio_request_duration_milliseconds_bucket:p95:rate1m"
          expr: |
            histogram_quantile(0.95,
              sum(irate(istio_request_duration_milliseconds_bucket{reporter="source"}[1m]))
              by (
                destination_canonical_service,
                destination_workload_namespace,
                source_canonical_service,
                source_workload_namespace,
                le
              )
            )
  rules: |
    {}
