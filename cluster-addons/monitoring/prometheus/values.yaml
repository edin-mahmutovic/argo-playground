kube-prometheus-stack:
  defaultRules:
    create: false
    rules:
      alertmanager: true
      etcd: true
      general: true
      k8s: true
      kubeApiserver: true
      kubeApiserverAvailability: true
      kubeApiserverError: true
      kubeApiserverSlos: true
      kubelet: true
      kubePrometheusGeneral: true
      kubePrometheusNodeAlerting: true
      kubePrometheusNodeRecording: true
      kubernetesAbsent: true
      kubernetesApps: true
      kubernetesResources: true
      kubernetesStorage: false
      kubernetesSystem: true
      kubeScheduler: true
      kubeStateMetrics: true
      network: true
      node: true
      prometheus: true
      prometheusOperator: true
      time: true

    ## Runbook url prefix for default rules
    runbookUrl: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#
    ## Reduce app namespace alert scope
    appNamespacesTarget: "*"

    ## Labels for default rules
    labels: {}
    ## Annotations for default rules
    annotations: {}

    ## Additional labels for PrometheusRule alerts
    additionalRuleLabels: {}
    ## Provide custom recording or alerting rules to be deployed into the cluster.
    ##
    additionalPrometheusRulesMap: {}
    #  rule-name:
    #    groups:
    #    - name: my_group
    #      rules:
    #      - record: my_record
    #        expr: 100 * my_record
  global:
    rbac:
      create: true
      pspEnabled: false
      pspAnnotations: {}
        ## Specify pod annotations
        ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#apparmor
        ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#seccomp
        ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#sysctl
        ##
        # seccomp.security.alpha.kubernetes.io/allowedProfileNames: '*'
        # seccomp.security.alpha.kubernetes.io/defaultProfileName: 'docker/default'
        # apparmor.security.beta.kubernetes.io/defaultProfileName: 'runtime/default'

  ## Configuration for alertmanager
  ## ref: https://prometheus.io/docs/alerting/alertmanager/
  ##
  alertmanager:

    ## Deploy alertmanager
    ##
    enabled: false

  grafana:
    enabled: false

  kubeControllerManager:
    enabled: false

  nodeExporter:
    enabled: false

  ## Manages Prometheus and Alertmanager components
  ##
  prometheusOperator:
    enabled: true

    # Prometheus-Operator v0.39.0 and later support TLS natively.
    tls:
      enabled: true
      # Value must match version names from https://golang.org/pkg/crypto/tls/#pkg-constants
      tlsMinVersion: VersionTLS13
      # The default webhook port is 10250 in order to work out-of-the-box in GKE private clusters and avoid adding firewall rules.
      internalPort: 10250

    ## Admission webhook support for PrometheusRules resources added in Prometheus Operator 0.30 can be enabled to prevent incorrectly formatted
    ## rules from making their way into prometheus and potentially preventing the container from starting
    admissionWebhooks:
      failurePolicy: Fail
      enabled: true
      ## A PEM encoded CA bundle which will be used to validate the webhook's server certificate.
      ## If unspecified, system trust roots on the apiserver are used.
      caBundle: ""
      ## If enabled, generate a self-signed certificate, then patch the webhook configurations with the generated data.
      ## On chart upgrades (or if the secret exists) the cert will not be re-generated. You can use this to provide your own
      ## certs ahead of time if you wish.
      ##
      patch:
        enabled: true
        image:
          repository: jettech/kube-webhook-certgen
          tag: v1.5.0
          sha: ""
          pullPolicy: IfNotPresent
        resources: {}
        ## Provide a priority class name to the webhook patching job
        ##
        priorityClassName: ""
        podAnnotations: {}
        nodeSelector: {}
        affinity: {}
        tolerations: []
      # Use certmanager to generate webhook certs
      certManager:
        enabled: false
        # issuerRef:
        #   name: "issuer"
        #   kind: "ClusterIssuer"

    ## Namespaces to scope the interaction of the Prometheus Operator and the apiserver (allow list).
    ## This is mutually exclusive with denyNamespaces. Setting this to an empty object will disable the configuration
    ##
    namespaces: []
      # releaseNamespace: true
      # additional:
      # - kube-system
      # - gitlab-stage
      # - gitlab-prod
      # - atlantis
      # - flux-system

    ## Namespaces not to scope the interaction of the Prometheus Operator (deny list).
    ##
    denyNamespaces: []

    ## Filter namespaces to look for prometheus-operator custom resources
    ##
    alertmanagerInstanceNamespaces: []
    prometheusInstanceNamespaces: []
    thanosRulerInstanceNamespaces: []

    ## Service account for Alertmanager to use.
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
    ##
    serviceAccount:
      create: true
      name: ""

    ## Configuration for Prometheus operator service
    ##
    service:
      annotations: {}
      labels: {}
      clusterIP: ""

    ## Port to expose on each node
    ## Only used if service.type is 'NodePort'
    ##
      nodePort: 30080

      nodePortTls: 30443

    ## Additional ports to open for Prometheus service
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#multi-port-services
    ##
      additionalPorts: []

    ## Loadbalancer IP
    ## Only use if service.type is "loadbalancer"
    ##
      loadBalancerIP: ""
      loadBalancerSourceRanges: []

    ## Service type
    ## NodePort, ClusterIP, loadbalancer
    ##
      type: ClusterIP

      ## List of IP addresses at which the Prometheus server service is available
      ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
      ##
      externalIPs: []

    ## Labels to add to the operator pod
    ##
    podLabels: {}

    ## Annotations to add to the operator pod
    ##
    podAnnotations: {}

    ## Assign a PriorityClassName to pods if set
    # priorityClassName: ""

    ## Define Log Format
    # Use logfmt (default) or json logging
    # logFormat: logfmt

    ## Decrease log verbosity to errors only
    # logLevel: error

    ## If true, the operator will create and maintain a service for scraping kubelets
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/master/helm/prometheus-operator/README.md
    ##
    kubeletService:
      enabled: true
      namespace: kube-system

    ## Create a servicemonitor for the operator
    ##
    serviceMonitor:
      ## Scrape interval. If not set, the Prometheus default scrape interval is used.
      ##
      interval: ""
      ## Scrape timeout. If not set, the Prometheus default scrape timeout is used.
      scrapeTimeout: ""
      selfMonitor: true

      ## 	metric relabel configs to apply to samples before ingestion.
      ##
      metricRelabelings: []
      # - action: keep
      #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
      #   sourceLabels: [__name__]

      # 	relabel configs to apply to samples before ingestion.
      ##
      relabelings: []
      # - sourceLabels: [__meta_kubernetes_pod_node_name]
      #   separator: ;
      #   regex: ^(.*)$
      #   targetLabel: nodename
      #   replacement: $1
      #   action: replace

    ## Resource limits & requests
    ##
    resources:
      limits:
        cpu: 200m
        memory: 200Mi
      requests:
        cpu: 100m
        memory: 100Mi

    # Required for use in managed kubernetes clusters (such as AWS EKS) with custom CNI (such as calico),
    # because control-plane managed by AWS cannot communicate with pods' IP CIDR and admission webhooks are not working
    ##
    hostNetwork: false

    ## Define which Nodes the Pods are scheduled on.
    ## ref: https://kubernetes.io/docs/user-guide/node-selection/
    ##
    nodeSelector: {}

    ## Tolerations for use with node taints
    ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
    ##
    tolerations: []
    # - key: "key"
    #   operator: "Equal"
    #   value: "value"
    #   effect: "NoSchedule"

    ## Assign custom affinity rules to the prometheus operator
    ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
    ##
    affinity: {}
      # nodeAffinity:
      #   requiredDuringSchedulingIgnoredDuringExecution:
      #     nodeSelectorTerms:
      #     - matchExpressions:
      #       - key: kubernetes.io/e2e-az-name
      #         operator: In
      #         values:
      #         - e2e-az1
      #         - e2e-az2

    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534

    ## Prometheus-operator image
    ##
    image:
      repository: quay.io/prometheus-operator/prometheus-operator
      tag: v0.45.0
      sha: ""
      pullPolicy: IfNotPresent

    ## Prometheus image to use for prometheuses managed by the operator
    ##
    # prometheusDefaultBaseImage: quay.io/prometheus/prometheus

    ## Alertmanager image to use for alertmanagers managed by the operator
    ##
    # alertmanagerDefaultBaseImage: quay.io/prometheus/alertmanager

    ## Prometheus-config-reloader image to use for config and rule reloading
    ##
    prometheusConfigReloaderImage:
      repository: quay.io/prometheus-operator/prometheus-config-reloader
      tag: v0.45.0
      sha: ""

    ## Set the prometheus config reloader side-car CPU limit
    ##
    configReloaderCpu: 100m

    ## Set the prometheus config reloader side-car memory limit
    ##
    configReloaderMemory: 50Mi

    ## Set a Field Selector to filter watched secrets
    ##
    secretFieldSelector: ""

  ## Deploy a Prometheus instance
  ##
  prometheus:
    enabled: false

prometheus-community:
  rbac:
    create: true
  serviceAccounts:
    server:
      create: true
    nodeExporter:
      create: true
    pushgateway:
      create: true
    alertmanager:
      create: true
  alertmanager:
    enabled: true
    baseURL: "http://localhost:9093"
    configMapOverrideName: "alertmanager-config"
    resources:
      limits:
        cpu: 10m
        memory: 32Mi
      requests:
        cpu: 10m
        memory: 32Mi
  nodeExporter:
    enabled: true
    resources:
      limits:
        cpu: 200m
        memory: 50Mi
      requests:
        cpu: 100m
        memory: 30Mi
  server:
    configMapOverrideName: "server-config"
    retention: "7d"
    enabled: true
    resources:
      limits:
        cpu: 1500m
        memory: 2Gi
      requests:
        cpu: 500m
        memory: 512Mi
  pushgateway:
    image:
      repository: quay.io/prometheus/pushgateway
      tag: v1.4.0
      pullPolicy: IfNotPresent
    enabled: true
    resources:
      limits:
        cpu: 10m
        memory: 32Mi
      requests:
        cpu: 10m
        memory: 32Mi

prometheus-adapter:
  prometheus:
    # Value is templated
    url: "http://kube-prometheus-stack-prometheus.metrics"
    port: 9090
    path: ""
  replicas: 1
  rbac:
    # Specifies whether RBAC resources should be created
    create: true
  serviceAccount:
    create: true
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      cpu: 100m
      memory: 128Mi
  rules:
    default: true
    custom: []
  # - seriesQuery: '{__name__=~"^some_metric_count$"}'
  #   resources:
  #     template: <<.Resource>>
  #   name:
  #     matches: ""
  #     as: "my_custom_metric"
  #   metricsQuery: sum(<<.Series>>{<<.LabelMatchers>>}) by (<<.GroupBy>>)
    # Mounts a configMap with pre-generated rules for use. Overrides the
    # default, custom, external and resource entries
    existing:
    external: []
  # - seriesQuery: '{__name__=~"^some_metric_count$"}'
  #   resources:
  #     template: <<.Resource>>
  #   name:
  #     matches: ""
  #     as: "my_external_metric"
  #   metricsQuery: sum(<<.Series>>{<<.LabelMatchers>>}) by (<<.GroupBy>>)
    resource: {}
  #   cpu:
  #     containerQuery: sum(rate(container_cpu_usage_seconds_total{<<.LabelMatchers>>}[3m])) by (<<.GroupBy>>)
  #     nodeQuery: sum(rate(container_cpu_usage_seconds_total{<<.LabelMatchers>>, id='/'}[3m])) by (<<.GroupBy>>)
  #     resources:
  #       overrides:
  #         instance:
  #           resource: node
  #         namespace:
  #           resource: namespace
  #         pod:
  #           resource: pod
  #     containerLabel: container
  #   memory:
  #     containerQuery: sum(container_memory_working_set_bytes{<<.LabelMatchers>>}) by (<<.GroupBy>>)
  #     nodeQuery: sum(container_memory_working_set_bytes{<<.LabelMatchers>>,id='/'}) by (<<.GroupBy>>)
  #     resources:
  #       overrides:
  #         instance:
  #           resource: node
  #         namespace:
  #           resource: namespace
  #         pod:
  #           resource: pod
  #     containerLabel: container
  #   window: 3m
  service:
    annotations: {}
    port: 443
    type: ClusterIP